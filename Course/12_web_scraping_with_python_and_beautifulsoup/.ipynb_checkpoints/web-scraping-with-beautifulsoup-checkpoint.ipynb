{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping With Python And BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand the benefits and use cases of web scraping.\n",
    "- Learn how to parse the HTML content of a webpage using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/doc) to extract specific elements.\n",
    "- Learn how to scan the HTML for specific keywords.\n",
    "- Learn how to scrape multiple web pages.\n",
    "- Learn how to store your web scraped data into a pandas dataframe.\n",
    "- Learn how to save the web scraped data as a local .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following installations are for a Jupyter Notebook, however if you are using a command line then simply <strong> exclude the ! symbol </strong>\n",
    "\n",
    "~~~\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Learn Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning web scraping is a useful skill, whether you work as a programmer, marketer or analyst. Its a fantastic way for you to analyse websites. Web scraping should never replace a tool such as [ScreamingFrog](https://www.screamingfrog.co.uk/seo-spider/), however when you're creating data pipelines with Python or JavaScript scripts, then you'll likely want to write a custom scraper.\n",
    "\n",
    "Because what's the point of doing a website crawl if you only need a few pieces of information per page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have acquired advanced web scraping skills, you can:\n",
    "    \n",
    "- Accurately monitor your competitors.\n",
    "- Create data pipelines that push fresh HTML data into a data warehouse such as [BigQuery.](https://cloud.google.com/bigquery)\n",
    "- Allow you to blend it with other data sources such as [Google Search Console](https://search.google.com/search-console/about) or [Google Analytics](https://analytics.google.com/analytics/web/) data.\n",
    "- Create your own APIs for websites that don't publicly expose an API.\n",
    "\n",
    "There are many other uses for why [web scraping](https://understandingdata.com/what-is-web-scraping/) is a powerful skill to possess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly every website is different, this means it can be difficult to build a robust web scraper that will work on every website. You'll likely need to create unique selectors for each website which can be time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, your scripts are more likely to fail over time because websites change. Whenever a marketer, owner or developer makes changes to their website, it could lead to your script breaking. Therefore for larger proejcts its essential that you create a monitoring system so that you can fix these problems as they arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape A Single HTML Page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape a web page in python or any programming language, we will need to download the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library that we'll be using is [requests](https://requests.readthedocs.io/en/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.co.uk/jobs?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the status code is 200 (which means Ok), then we'll be able to access the web page. You can always check the status code with:\n",
    "\n",
    "~~~\n",
    "\n",
    "print(response.status_code)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the content of a request, simply use:\n",
    "    \n",
    "~~~\n",
    "\n",
    "response.content\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will store the HTML content as a stream of bytes:\n",
    "html_content = response.content\n",
    "\n",
    "# This will store the HTML content as a string:\n",
    "html_content_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the HTML Content to a Parser\n",
    "\n",
    "Simply downloading the HTML page is not enough, particularly if we would like to extract elements from it. Therefore we will use a python package called [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). BeautifulSoup provides us with a large amount of DOM (document object model) parsing methods. \n",
    "\n",
    "In order to parse the DOM of a page, simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BeautifulSoup in module bs4 object:\n",
      "\n",
      "class BeautifulSoup(bs4.element.Tag)\n",
      " |  BeautifulSoup(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      " |  \n",
      " |  A data structure representing a parsed HTML or XML document.\n",
      " |  \n",
      " |  Most of the methods you'll call on a BeautifulSoup object are inherited from\n",
      " |  PageElement or Tag.\n",
      " |  \n",
      " |  Internally, this class defines the basic interface called by the\n",
      " |  tree builders when converting an HTML/XML document into a data\n",
      " |  structure. The interface abstracts away the differences between\n",
      " |  parsers. To write a new tree builder, you'll need to understand\n",
      " |  these methods as a whole.\n",
      " |  \n",
      " |  These methods will be called by the BeautifulSoup constructor:\n",
      " |    * reset()\n",
      " |    * feed(markup)\n",
      " |  \n",
      " |  The tree builder may call these methods from its feed() implementation:\n",
      " |    * handle_starttag(name, attrs) # See note about return value\n",
      " |    * handle_endtag(name)\n",
      " |    * handle_data(data) # Appends to the current data node\n",
      " |    * endData(containerClass) # Ends the current data node\n",
      " |  \n",
      " |  No matter how complicated the underlying parser is, you should be\n",
      " |  able to build a tree using 'start tag' events, 'end tag' events,\n",
      " |  'data' events, and \"done with data\" events.\n",
      " |  \n",
      " |  If you encounter an empty-element tag (aka a self-closing tag,\n",
      " |  like HTML's <br> tag), call handle_starttag and then\n",
      " |  handle_endtag.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BeautifulSoup\n",
      " |      bs4.element.Tag\n",
      " |      bs4.element.PageElement\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |      Copy a BeautifulSoup object by converting the document to a string and parsing it again.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      " |      Constructor.\n",
      " |      \n",
      " |      :param markup: A string or a file-like object representing\n",
      " |       markup to be parsed.\n",
      " |      \n",
      " |      :param features: Desirable features of the parser to be\n",
      " |       used. This may be the name of a specific parser (\"lxml\",\n",
      " |       \"lxml-xml\", \"html.parser\", or \"html5lib\") or it may be the\n",
      " |       type of markup to be used (\"html\", \"html5\", \"xml\"). It's\n",
      " |       recommended that you name a specific parser, so that\n",
      " |       Beautiful Soup gives you the same results across platforms\n",
      " |       and virtual environments.\n",
      " |      \n",
      " |      :param builder: A TreeBuilder subclass to instantiate (or\n",
      " |       instance to use) instead of looking one up based on\n",
      " |       `features`. You only need to use this if you've implemented a\n",
      " |       custom TreeBuilder.\n",
      " |      \n",
      " |      :param parse_only: A SoupStrainer. Only parts of the document\n",
      " |       matching the SoupStrainer will be considered. This is useful\n",
      " |       when parsing part of a document that would otherwise be too\n",
      " |       large to fit into memory.\n",
      " |      \n",
      " |      :param from_encoding: A string indicating the encoding of the\n",
      " |       document to be parsed. Pass this in if Beautiful Soup is\n",
      " |       guessing wrongly about the document's encoding.\n",
      " |      \n",
      " |      :param exclude_encodings: A list of strings indicating\n",
      " |       encodings known to be wrong. Pass this in if you don't know\n",
      " |       the document's encoding but you know Beautiful Soup's guess is\n",
      " |       wrong.\n",
      " |      \n",
      " |      :param element_classes: A dictionary mapping BeautifulSoup\n",
      " |       classes like Tag and NavigableString, to other classes you'd\n",
      " |       like to be instantiated instead as the parse tree is\n",
      " |       built. This is useful for subclassing Tag or NavigableString\n",
      " |       to modify default behavior.\n",
      " |      \n",
      " |      :param kwargs: For backwards compatibility purposes, the\n",
      " |       constructor accepts certain keyword arguments used in\n",
      " |       Beautiful Soup 3. None of these arguments do anything in\n",
      " |       Beautiful Soup 4; they will result in a warning and then be\n",
      " |       ignored.\n",
      " |       \n",
      " |       Apart from this, any keyword arguments passed into the\n",
      " |       BeautifulSoup constructor are propagated to the TreeBuilder\n",
      " |       constructor. This makes it possible to configure a\n",
      " |       TreeBuilder by passing in arguments, not just by saying which\n",
      " |       one to use.\n",
      " |  \n",
      " |  decode(self, pretty_print=False, eventual_encoding='utf-8', formatter='minimal')\n",
      " |      Returns a string or Unicode representation of the parse tree\n",
      " |          as an HTML or XML document.\n",
      " |      \n",
      " |      :param pretty_print: If this is True, indentation will be used to\n",
      " |          make the document more readable.\n",
      " |      :param eventual_encoding: The encoding of the final document.\n",
      " |          If this is None, the document will be a Unicode string.\n",
      " |  \n",
      " |  endData(self, containerClass=None)\n",
      " |      Method called by the TreeBuilder when the end of a data segment\n",
      " |      occurs.\n",
      " |  \n",
      " |  handle_data(self, data)\n",
      " |      Called by the tree builder when a chunk of textual data is encountered.\n",
      " |  \n",
      " |  handle_endtag(self, name, nsprefix=None)\n",
      " |      Called by the tree builder when an ending tag is encountered.\n",
      " |      \n",
      " |      :param name: Name of the tag.\n",
      " |      :param nsprefix: Namespace prefix for the tag.\n",
      " |  \n",
      " |  handle_starttag(self, name, namespace, nsprefix, attrs, sourceline=None, sourcepos=None)\n",
      " |      Called by the tree builder when a new tag is encountered.\n",
      " |      \n",
      " |      :param name: Name of the tag.\n",
      " |      :param nsprefix: Namespace prefix for the tag.\n",
      " |      :param attrs: A dictionary of attribute values.\n",
      " |      :param sourceline: The line number where this tag was found in its\n",
      " |          source document.\n",
      " |      :param sourcepos: The character position within `sourceline` where this\n",
      " |          tag was found.\n",
      " |      \n",
      " |      If this method returns None, the tag was rejected by an active\n",
      " |      SoupStrainer. You should proceed as if the tag had not occurred\n",
      " |      in the document. For instance, if this was a self-closing tag,\n",
      " |      don't call handle_endtag.\n",
      " |  \n",
      " |  insert_after(self, *args)\n",
      " |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      " |      it because there is nothing before or after it in the parse tree.\n",
      " |  \n",
      " |  insert_before(self, *args)\n",
      " |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      " |      it because there is nothing before or after it in the parse tree.\n",
      " |  \n",
      " |  new_string(self, s, subclass=None)\n",
      " |      Create a new NavigableString associated with this BeautifulSoup\n",
      " |      object.\n",
      " |  \n",
      " |  new_tag(self, name, namespace=None, nsprefix=None, attrs={}, sourceline=None, sourcepos=None, **kwattrs)\n",
      " |      Create a new Tag associated with this BeautifulSoup object.\n",
      " |      \n",
      " |      :param name: The name of the new Tag.\n",
      " |      :param namespace: The URI of the new Tag's XML namespace, if any.\n",
      " |      :param prefix: The prefix for the new Tag's XML namespace, if any.\n",
      " |      :param attrs: A dictionary of this Tag's attribute values; can\n",
      " |          be used instead of `kwattrs` for attributes like 'class'\n",
      " |          that are reserved words in Python.\n",
      " |      :param sourceline: The line number where this tag was\n",
      " |          (purportedly) found in its source document.\n",
      " |      :param sourcepos: The character position within `sourceline` where this\n",
      " |          tag was (purportedly) found.\n",
      " |      :param kwattrs: Keyword arguments for the new Tag's attribute values.\n",
      " |  \n",
      " |  object_was_parsed(self, o, parent=None, most_recent_element=None)\n",
      " |      Method called by the TreeBuilder to integrate an object into the parse tree.\n",
      " |  \n",
      " |  popTag(self)\n",
      " |      Internal method called by _popToTag when a tag is closed.\n",
      " |  \n",
      " |  pushTag(self, tag)\n",
      " |      Internal method called by handle_starttag when a tag is opened.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Reset this object to a state as though it had never parsed any\n",
      " |      markup.\n",
      " |  \n",
      " |  string_container(self, base_class=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ASCII_SPACES = ' \\n\\t\\x0c\\r'\n",
      " |  \n",
      " |  DEFAULT_BUILDER_FEATURES = ['html', 'fast']\n",
      " |  \n",
      " |  NO_PARSER_SPECIFIED_WARNING = 'No parser was explicitly specified, so ...\n",
      " |  \n",
      " |  ROOT_TAG_NAME = '[document]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from bs4.element.Tag:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      A tag is non-None even if it has no contents.\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Calling a Tag like a function is the same as calling its\n",
      " |      find_all() method. Eg. tag('a') returns a list of all the A tags\n",
      " |      found within this tag.\n",
      " |  \n",
      " |  __contains__(self, x)\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |      Deleting tag[key] deletes all 'key' attributes for the tag.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Returns true iff this Tag has the same name, the same attributes,\n",
      " |      and the same contents (recursively) as `other`.\n",
      " |  \n",
      " |  __getattr__(self, tag)\n",
      " |      Calling tag.subtag is the same as calling tag.find(name=\"subtag\")\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      tag[key] returns the value of the 'key' attribute for the Tag,\n",
      " |      and throws an exception if it's not there.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterating over a Tag iterates over its contents.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      The length of a Tag is the length of its list of contents.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Returns true iff this Tag is not identical to `other`,\n",
      " |      as defined in __eq__.\n",
      " |  \n",
      " |  __repr__ = __unicode__(self)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |      Setting tag[key] sets the value of the 'key' attribute for the\n",
      " |      tag.\n",
      " |  \n",
      " |  __str__ = __unicode__(self)\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |      Renders this PageElement as a Unicode string.\n",
      " |  \n",
      " |  childGenerator(self)\n",
      " |      Deprecated generator.\n",
      " |  \n",
      " |  clear(self, decompose=False)\n",
      " |      Wipe out all children of this PageElement by calling extract()\n",
      " |         on them.\n",
      " |      \n",
      " |      :param decompose: If this is True, decompose() (a more\n",
      " |          destructive method) will be called instead of extract().\n",
      " |  \n",
      " |  decode_contents(self, indent_level=None, eventual_encoding='utf-8', formatter='minimal')\n",
      " |      Renders the contents of this tag as a Unicode string.\n",
      " |      \n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |         indented this many spaces. Used internally in\n",
      " |         recursive calls while pretty-printing.\n",
      " |      \n",
      " |      :param eventual_encoding: The tag is destined to be\n",
      " |         encoded into this encoding. decode_contents() is _not_\n",
      " |         responsible for performing that encoding. This information\n",
      " |         is passed in so that it can be substituted in if the\n",
      " |         document contains a <META> tag that mentions the document's\n",
      " |         encoding.\n",
      " |      \n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard Formatters.\n",
      " |  \n",
      " |  decompose(self)\n",
      " |      Recursively destroys this PageElement and its children.\n",
      " |      \n",
      " |      This element will be removed from the tree and wiped out; so\n",
      " |      will everything beneath it.\n",
      " |      \n",
      " |      The behavior of a decomposed PageElement is undefined and you\n",
      " |      should never use one for anything, but if you need to _check_\n",
      " |      whether an element has been decomposed, you can use the\n",
      " |      `decomposed` property.\n",
      " |  \n",
      " |  encode(self, encoding='utf-8', indent_level=None, formatter='minimal', errors='xmlcharrefreplace')\n",
      " |      Render a bytestring representation of this PageElement and its\n",
      " |      contents.\n",
      " |      \n",
      " |      :param encoding: The destination encoding.\n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |          indented this many spaces. Used internally in\n",
      " |          recursive calls while pretty-printing.\n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard formatters.\n",
      " |      :param errors: An error handling strategy such as\n",
      " |          'xmlcharrefreplace'. This value is passed along into\n",
      " |          encode() and its value should be one of the constants\n",
      " |          defined by Python.\n",
      " |      :return: A bytestring.\n",
      " |  \n",
      " |  encode_contents(self, indent_level=None, encoding='utf-8', formatter='minimal')\n",
      " |      Renders the contents of this PageElement as a bytestring.\n",
      " |      \n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |         indented this many spaces. Used internally in\n",
      " |         recursive calls while pretty-printing.\n",
      " |      \n",
      " |      :param eventual_encoding: The bytestring will be in this encoding.\n",
      " |      \n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard Formatters.\n",
      " |      \n",
      " |      :return: A bytestring.\n",
      " |  \n",
      " |  find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
      " |      Look in the children of this PageElement and find the first\n",
      " |      PageElement that matches the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param recursive: If this is True, find() will perform a\n",
      " |          recursive search of this PageElement's children. Otherwise,\n",
      " |          only the direct children will be considered.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  findAll = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findChild = find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
      " |  \n",
      " |  findChildren = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |      Look in the children of this PageElement and find all\n",
      " |      PageElements that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param recursive: If this is True, find_all() will perform a\n",
      " |          recursive search of this PageElement's children. Otherwise,\n",
      " |          only the direct children will be considered.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      Returns the value of the 'key' attribute for the tag, or\n",
      " |      the value given for 'default' if it doesn't have that\n",
      " |      attribute.\n",
      " |  \n",
      " |  getText = get_text(self, separator='', strip=False, types=(<class 'bs4.element.NavigableString'>, <class 'bs4.element.CData'>))\n",
      " |  \n",
      " |  get_attribute_list(self, key, default=None)\n",
      " |      The same as get(), but always returns a list.\n",
      " |      \n",
      " |      :param key: The attribute to look for.\n",
      " |      :param default: Use this value if the attribute is not present\n",
      " |          on this PageElement.\n",
      " |      :return: A list of values, probably containing only a single\n",
      " |          value.\n",
      " |  \n",
      " |  get_text(self, separator='', strip=False, types=(<class 'bs4.element.NavigableString'>, <class 'bs4.element.CData'>))\n",
      " |      Get all child strings, concatenated using the given separator.\n",
      " |      \n",
      " |      :param separator: Strings will be concatenated using this separator.\n",
      " |      \n",
      " |      :param strip: If True, strings will be stripped before being\n",
      " |          concatenated.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          stylesheets, etc.\n",
      " |      \n",
      " |      :return: A string.\n",
      " |  \n",
      " |  has_attr(self, key)\n",
      " |      Does this PageElement have an attribute with the given name?\n",
      " |  \n",
      " |  has_key(self, key)\n",
      " |      Deprecated method. This was kind of misleading because has_key()\n",
      " |      (attributes) was different from __in__ (contents).\n",
      " |      \n",
      " |      has_key() is gone in Python 3, anyway.\n",
      " |  \n",
      " |  index(self, element)\n",
      " |      Find the index of a child by identity, not value.\n",
      " |      \n",
      " |      Avoids issues with tag.contents.index(element) getting the\n",
      " |      index of equal elements.\n",
      " |      \n",
      " |      :param element: Look for this PageElement in `self.contents`.\n",
      " |  \n",
      " |  prettify(self, encoding=None, formatter='minimal')\n",
      " |      Pretty-print this PageElement as a string.\n",
      " |      \n",
      " |      :param encoding: The eventual encoding of the string. If this is None,\n",
      " |          a Unicode string will be returned.\n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard formatters.\n",
      " |      :return: A Unicode string (if encoding==None) or a bytestring \n",
      " |          (otherwise).\n",
      " |  \n",
      " |  recursiveChildGenerator(self)\n",
      " |      Deprecated generator.\n",
      " |  \n",
      " |  renderContents(self, encoding='utf-8', prettyPrint=False, indentLevel=0)\n",
      " |      Deprecated method for BS3 compatibility.\n",
      " |  \n",
      " |  select(self, selector, namespaces=None, limit=None, **kwargs)\n",
      " |      Perform a CSS selection operation on the current element.\n",
      " |      \n",
      " |      This uses the SoupSieve library.\n",
      " |      \n",
      " |      :param selector: A string containing a CSS selector.\n",
      " |      \n",
      " |      :param namespaces: A dictionary mapping namespace prefixes\n",
      " |         used in the CSS selector to namespace URIs. By default,\n",
      " |         Beautiful Soup will use the prefixes it encountered while\n",
      " |         parsing the document.\n",
      " |      \n",
      " |      :param limit: After finding this number of results, stop looking.\n",
      " |      \n",
      " |      :param kwargs: Keyword arguments to be passed into SoupSieve's \n",
      " |         soupsieve.select() method.\n",
      " |      \n",
      " |      :return: A ResultSet of Tags.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  select_one(self, selector, namespaces=None, **kwargs)\n",
      " |      Perform a CSS selection operation on the current element.\n",
      " |      \n",
      " |      :param selector: A CSS selector.\n",
      " |      \n",
      " |      :param namespaces: A dictionary mapping namespace prefixes\n",
      " |         used in the CSS selector to namespace URIs. By default,\n",
      " |         Beautiful Soup will use the prefixes it encountered while\n",
      " |         parsing the document.\n",
      " |      \n",
      " |      :param kwargs: Keyword arguments to be passed into SoupSieve's \n",
      " |         soupsieve.select() method.\n",
      " |      \n",
      " |      :return: A Tag.\n",
      " |      :rtype: bs4.element.Tag\n",
      " |  \n",
      " |  smooth(self)\n",
      " |      Smooth out this element's children by consolidating consecutive\n",
      " |      strings.\n",
      " |      \n",
      " |      This makes pretty-printed output look more natural following a\n",
      " |      lot of operations that modified the tree.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from bs4.element.Tag:\n",
      " |  \n",
      " |  children\n",
      " |      Iterate over all direct children of this PageElement.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  descendants\n",
      " |      Iterate over all children of this PageElement in a\n",
      " |      breadth-first sequence.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  isSelfClosing\n",
      " |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      " |      \n",
      " |      A tag that has contents is never an empty-element tag.\n",
      " |      \n",
      " |      A tag that has no contents may or may not be an empty-element\n",
      " |      tag. It depends on the builder used to create the tag. If the\n",
      " |      builder has a designated list of empty-element tags, then only\n",
      " |      a tag whose name shows up in that list is considered an\n",
      " |      empty-element tag.\n",
      " |      \n",
      " |      If the builder has no designated list of empty-element tags,\n",
      " |      then any tag with no contents is an empty-element tag.\n",
      " |  \n",
      " |  is_empty_element\n",
      " |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      " |      \n",
      " |      A tag that has contents is never an empty-element tag.\n",
      " |      \n",
      " |      A tag that has no contents may or may not be an empty-element\n",
      " |      tag. It depends on the builder used to create the tag. If the\n",
      " |      builder has a designated list of empty-element tags, then only\n",
      " |      a tag whose name shows up in that list is considered an\n",
      " |      empty-element tag.\n",
      " |      \n",
      " |      If the builder has no designated list of empty-element tags,\n",
      " |      then any tag with no contents is an empty-element tag.\n",
      " |  \n",
      " |  parserClass\n",
      " |  \n",
      " |  string\n",
      " |      Convenience property to get the single string within this\n",
      " |      PageElement.\n",
      " |      \n",
      " |      TODO It might make sense to have NavigableString.string return\n",
      " |      itself.\n",
      " |      \n",
      " |      :return: If this element has a single string child, return\n",
      " |       value is that string. If this element has one child tag,\n",
      " |       return value is the 'string' attribute of the child tag,\n",
      " |       recursively. If this element is itself a string, has no\n",
      " |       children, or has more than one child, return value is None.\n",
      " |  \n",
      " |  strings\n",
      " |      Yield all strings of certain classes, possibly stripping them.\n",
      " |      \n",
      " |      :param strip: If True, all strings will be stripped before being\n",
      " |          yielded.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          etc.\n",
      " |      \n",
      " |      :yield: A sequence of strings.\n",
      " |  \n",
      " |  stripped_strings\n",
      " |      Yield all strings in the document, stripping them first.\n",
      " |      \n",
      " |      :yield: A sequence of stripped strings.\n",
      " |  \n",
      " |  text\n",
      " |      Get all child strings, concatenated using the given separator.\n",
      " |      \n",
      " |      :param separator: Strings will be concatenated using this separator.\n",
      " |      \n",
      " |      :param strip: If True, strings will be stripped before being\n",
      " |          concatenated.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          stylesheets, etc.\n",
      " |      \n",
      " |      :return: A string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from bs4.element.PageElement:\n",
      " |  \n",
      " |  append(self, tag)\n",
      " |      Appends the given PageElement to the contents of this one.\n",
      " |      \n",
      " |      :param tag: A PageElement.\n",
      " |  \n",
      " |  extend(self, tags)\n",
      " |      Appends the given PageElements to this one's contents.\n",
      " |      \n",
      " |      :param tags: A list of PageElements.\n",
      " |  \n",
      " |  extract(self, _self_index=None)\n",
      " |      Destructively rips this element out of the tree.\n",
      " |      \n",
      " |      :param _self_index: The location of this element in its parent's\n",
      " |         .contents, if known. Passing this in allows for a performance\n",
      " |         optimization.\n",
      " |      \n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  fetchNextSiblings = find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchPrevious = find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findAllNext = find_all_next(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findAllPrevious = find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findNext = find_next(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findNextSibling = find_next_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findNextSiblings = find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findParent = find_parent(self, name=None, attrs={}, **kwargs)\n",
      " |  \n",
      " |  findParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |  \n",
      " |  findPrevious = find_previous(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findPreviousSibling = find_previous_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  find_all_next(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Find all PageElements that match the given criteria and appear\n",
      " |      later in the document than this PageElement.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet containing PageElements.\n",
      " |  \n",
      " |  find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Look backwards in the document from this PageElement and find all\n",
      " |      PageElements that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  find_next(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Find the first PageElement that matches the given criteria and\n",
      " |      appears later in the document than this PageElement.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_next_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Find the closest sibling to this PageElement that matches the\n",
      " |      given criteria and appears later in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the\n",
      " |      online documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Find all siblings of this PageElement that match the given criteria\n",
      " |      and appear later in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  find_parent(self, name=None, attrs={}, **kwargs)\n",
      " |      Find the closest parent of this PageElement that matches the given\n",
      " |      criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |      Find all parents of this PageElement that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Look backwards in the document from this PageElement and find the\n",
      " |      first PageElement that matches the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Returns the closest sibling to this PageElement that matches the\n",
      " |      given criteria and appears earlier in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Returns all siblings to this PageElement that match the\n",
      " |      given criteria and appear earlier in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  format_string(self, s, formatter)\n",
      " |      Format the given string using the given formatter.\n",
      " |      \n",
      " |      :param s: A string.\n",
      " |      :param formatter: A Formatter object, or a string naming one of the standard formatters.\n",
      " |  \n",
      " |  formatter_for_name(self, formatter)\n",
      " |      Look up or create a Formatter for the given identifier,\n",
      " |      if necessary.\n",
      " |      \n",
      " |      :param formatter: Can be a Formatter object (used as-is), a\n",
      " |          function (used as the entity substitution hook for an\n",
      " |          XMLFormatter or HTMLFormatter), or a string (used to look\n",
      " |          up an XMLFormatter or HTMLFormatter in the appropriate\n",
      " |          registry.\n",
      " |  \n",
      " |  insert(self, position, new_child)\n",
      " |      Insert a new PageElement in the list of this PageElement's children.\n",
      " |      \n",
      " |      This works the same way as `list.insert`.\n",
      " |      \n",
      " |      :param position: The numeric position that should be occupied\n",
      " |         in `self.children` by the new PageElement. \n",
      " |      :param new_child: A PageElement.\n",
      " |  \n",
      " |  nextGenerator(self)\n",
      " |      # Old non-property versions of the generators, for backwards\n",
      " |      # compatibility with BS3.\n",
      " |  \n",
      " |  nextSiblingGenerator(self)\n",
      " |  \n",
      " |  parentGenerator(self)\n",
      " |  \n",
      " |  previousGenerator(self)\n",
      " |  \n",
      " |  previousSiblingGenerator(self)\n",
      " |  \n",
      " |  replaceWith = replace_with(self, replace_with)\n",
      " |  \n",
      " |  replaceWithChildren = unwrap(self)\n",
      " |  \n",
      " |  replace_with(self, replace_with)\n",
      " |      Replace this PageElement with another one, keeping the rest of the\n",
      " |      tree the same.\n",
      " |      \n",
      " |      :param replace_with: A PageElement.\n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  replace_with_children = unwrap(self)\n",
      " |  \n",
      " |  setup(self, parent=None, previous_element=None, next_element=None, previous_sibling=None, next_sibling=None)\n",
      " |      Sets up the initial relations between this element and\n",
      " |      other elements.\n",
      " |      \n",
      " |      :param parent: The parent of this element.\n",
      " |      \n",
      " |      :param previous_element: The element parsed immediately before\n",
      " |          this one.\n",
      " |      \n",
      " |      :param next_element: The element parsed immediately before\n",
      " |          this one.\n",
      " |      \n",
      " |      :param previous_sibling: The most recently encountered element\n",
      " |          on the same level of the parse tree as this one.\n",
      " |      \n",
      " |      :param previous_sibling: The next element to be encountered\n",
      " |          on the same level of the parse tree as this one.\n",
      " |  \n",
      " |  unwrap(self)\n",
      " |      Replace this PageElement with its contents.\n",
      " |      \n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  wrap(self, wrap_inside)\n",
      " |      Wrap this PageElement inside another one.\n",
      " |      \n",
      " |      :param wrap_inside: A PageElement.\n",
      " |      :return: `wrap_inside`, occupying the position in the tree that used\n",
      " |         to be occupied by `self`, and with `self` inside it.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from bs4.element.PageElement:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  decomposed\n",
      " |      Check whether a PageElement has been decomposed.\n",
      " |      \n",
      " |      :rtype: bool\n",
      " |  \n",
      " |  next\n",
      " |      The PageElement, if any, that was parsed just after this one.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  nextSibling\n",
      " |  \n",
      " |  next_elements\n",
      " |      All PageElements that were parsed after this one.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  next_siblings\n",
      " |      All PageElements that are siblings of this one but were parsed\n",
      " |      later.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  parents\n",
      " |      All PageElements that are parents of this PageElement.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  previous\n",
      " |      The PageElement, if any, that was parsed just before this one.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  previousSibling\n",
      " |  \n",
      " |  previous_elements\n",
      " |      All PageElements that were parsed before this one.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  previous_siblings\n",
      " |      All PageElements that are siblings of this one but were parsed\n",
      " |      earlier.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that instead of a HTML bytes string, <strong> we have a BeautifulSoup object</strong>, that has many functions on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we'll be web scraping indeed and extracting job information from [Indeed.co.uk](https://www.indeed.co.uk/)\n",
    "\n",
    "* The job will be: data scientist.\n",
    "* The area will be london."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate The URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url = 'https://www.indeed.co.uk/jobs?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a lot of information inside of a URL. \n",
    "\n",
    "Its important for you to be able to identify the structure of URLs and to reverse engineer how they might have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <strong> The base URL </strong> means the path to the jobs functionality of the website which in this case is: https://www.index.co.uk/\n",
    "2. <strong> Query Parameters </strong> are a way for the jobs search to be dynamic, in the above example they are: \n",
    "<strong> ?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query parameters consist of:\n",
    "- The start of the query at q\n",
    "- A key and value for each query parameter (i.e. l = london or start=40)\n",
    "- A separator which is an ampersand symbol (&) that separates all of the key + value query parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually Inspect The Webpage In Google Chrome Dev Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping straight into coding, its worthwhile visually inspecting the HTML page content within your browser. This will give you a sense of how the website is constructed and what repeating patterns you can see within the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Chrome Developer tools is a free available tool that allows you to visually inspect the HTML code. \n",
    "\n",
    "Navigate to it by:\n",
    "\n",
    "1. Opening up Google Chrome.\n",
    "2. Right clicking on a webpage.\n",
    "3. Clicking inspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://understandingdata.com/wp-content/uploads/2020/11/google-chrome-dev-tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://understandingdata.com/wp-content/uploads/2020/11/google-chrome-dev-tools-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Element By HTML ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to select specific HTML elements by using the <strong> #id CSS selector.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "appPromoBanner = soup.find('div', {'id':'appPromoBanner'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Element By HTML Class Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can find elements by their class selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_div = soup.find('div', class_='tab-container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(container_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Extract Text From HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as selecting the entire HTML element, you can also easily extract the text using BeautifulSoup. \n",
    "\n",
    "Let's see how this might work whilst scraping a single job advertisement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_url = 'https://www.indeed.co.uk/viewjob?cmp=Crowd-Link-Consulting&t=Business+Intelligence+Engineer&jk=9129263166da1718&q=data+engineer&vjs=3'\n",
    "resp = requests.get(job_url)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sempioneer.com/wp-content/uploads/2020/11/extracting-a-single-job-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting The Title Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's extract the title tag and then use .text to obtain the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag_text = soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Intelligence Engineer - Woking - Indeed.co.uk\n"
     ]
    }
   ],
   "source": [
    "print(title_tag_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can extract the first paragraph on the webpage, then get the text for that element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph = soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Business Intelligence Engineer  Woking, Surrey</b></p>\n"
     ]
    }
   ],
   "source": [
    "print(first_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll want to store multiple elements, for example if there is a list of job advertisements on the same page. The following method will return a list of elements rather than just the first element:\n",
    "\n",
    "~~~\n",
    "\n",
    "soup.findAll(some_element)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = soup.findAll('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p><b>Business Intelligence Engineer  Woking, Surrey</b></p>, <p><b>Objective </b></p>, <p>This role needs to work closely with our clients customers to turn data into critical information and knowledge that can be used to make sound business decisions. They provide data that is accurate, congruent, reliable and is easily accessible.</p>]\n"
     ]
    }
   ],
   "source": [
    "print(all_paragraphs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to extract the text of every paragraph element, we could just do a list compehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs_text = [paragraph.text.strip() for paragraph in all_paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to remove paragraph tags if they contain empty strings, by only including paragraphs which are truthy (don't have empty strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only return paragraphs that don't have empty strings!\n",
    "full_paragraphs = [paragraph for paragraph in all_paragraphs_text if paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(full_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape Multiple HTML Pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to web scrape multiple pages, then we'll simply create a for loop and multiple beautifulsoup objects.\n",
    "\n",
    "The important things are:\n",
    "    \n",
    "- Have a results dictionary or list(s) that is outside of the loop.\n",
    "- Extract either the result or N/A or a NaN (not a number), this is especially important when you're using python lists as it ensures that all of your python lists will always be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://understandingdata.com/',\n",
    "       'https://understandingdata.com/about-me/',\n",
    "       'https://understandingdata.com/contact/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a results list to store all of the web scraped data:\n",
    "results = []\n",
    "\n",
    "for url in urls:\n",
    "    # 2. Obtain the HTML response:\n",
    "    response = requests.get(url)\n",
    "    # 3. Create a BeautifulSoup object:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # 4. Extract the elements per URL:\n",
    "    title_tag = soup.title\n",
    "    results.append(title_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just Understanding Data', 'James Anthony Phoenix | Just Understanding Data', 'Contact | Just Understanding Data']\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![website titles extracted with beautifulsoup and python](https://sempioneer.com/wp-content/uploads/2020/11/website-titles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Scan HTML Content For Specific Keywords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularly in a marketing context, if one of your web pages is ranking for 5 keywords it would be beneficial to know:\n",
    "    \n",
    "- If every keyword was on a given HTML page.\n",
    "- If there were keywords on / missing from the HTML page.\n",
    "\n",
    "By writing a web scraper we can easily answer these questions at scale.\n",
    "\n",
    "----\n",
    "\n",
    "Let's say that our keyword is <strong> Understanding Data</strong>, we will normalise this to be lowercase with:\n",
    "\n",
    "~~~\n",
    "\n",
    "keyword = 'Understanding Data'.lower()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "\n",
    "keyword = 'Understanding Data'.lower()\n",
    "\n",
    "for url in urls:\n",
    "    # Creating a new item in the dictionary:\n",
    "    url_dict[url] = {'in_title': False, 'in_html': False}\n",
    "    \n",
    "    # Obtaining the HTML page with python requests:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract the HTML content into a string and normalise it to be lowercase:\n",
    "        cleaned_html_text = response.text.lower()\n",
    "        # Extract the HTML elements using BeautifulSoup:\n",
    "        title_tag = soup.title\n",
    "        # Checking to see if the keyword is present in the HTML and the <title> tag and the HTML content:\n",
    "        if keyword in title_tag:\n",
    "            url_dict[url]['in_title'] = True\n",
    "        if keyword in cleaned_html_text:\n",
    "            url_dict[url]['in_html'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://understandingdata.com/': {'in_title': False, 'in_html': True}, 'https://understandingdata.com/about-me/': {'in_title': False, 'in_html': True}, 'https://understandingdata.com/contact/': {'in_title': False, 'in_html': True}}\n"
     ]
    }
   ],
   "source": [
    "print(url_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![keyword in HTML content detection with python](https://sempioneer.com/wp-content/uploads/2020/11/keyword_in_html.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above, how easily it is to web scrape multiple pages and search the HTML content as well as the title tag.\n",
    "\n",
    "This can be extended to search many more HTML elements rather than just two.\n",
    "\n",
    "If we would like to do 30 or 50 it would be better to use this structure:\n",
    "    \n",
    "\n",
    "~~~\n",
    "\n",
    "for item_name, data in zip(['in_html', 'in_title'],[cleaned_html_text, title_tag]):\n",
    "    if keyword in data:\n",
    "        url_dict[url][item_name] = True\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Create A Pandas Dataframe From Web Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After web scraping and collecting your data from many web pages, it's ideal to store it within a pandas dataframe. From there you'll be able to push it directly to BigQuery or store it locally as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame()\n",
    "master_df = master_df.from_dict(url_dict, orient='index')\n",
    "\n",
    "# Resetting the index:\n",
    "master_df.reset_index(drop=False, inplace=True)\n",
    "master_df.rename(columns={'index': 'URL'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>in_title</th>\n",
       "      <th>in_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://understandingdata.com/</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://understandingdata.com/about-me/</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://understandingdata.com/contact/</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       URL  in_title  in_html\n",
       "0            http://understandingdata.com/     False     True\n",
       "1  https://understandingdata.com/about-me/     False     True\n",
       "2   https://understandingdata.com/contact/     False     True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how to store web scraped data in a pandas dataframe](https://sempioneer.com/wp-content/uploads/2020/11/how-to-store-data-into-a-pandas-dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Save The Web Scraped Data To A .CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is inside of a pandas dataframe, we can easily save it with the following method:\n",
    "    \n",
    "~~~\n",
    "\n",
    "master_df.to_csv(file_name.csv)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv('web_scraped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this tutorial has sparked your curiosity with web scraping, I'd recommend reviewing the following resources to learn more:\n",
    "\n",
    "- [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/doc)\n",
    "- [Python Requests documentation](https://requests.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
