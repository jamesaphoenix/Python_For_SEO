{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Keyword De-duplication + 2. Comparing Keyword Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn how to de-duplicate keyword lists using set operations\n",
    "- Learn how to compare two lists of keywords to using set operations\n",
    "- Learn how to de-duplicate keyword lists using FuzzyWuzzy\n",
    "- Learn how to open an Ahrefs keyword report .CSV file with Pandas\n",
    "- Performing standard data analysis operations within Pandas on a keyword report from Ahrefs (including GroupBy Objects, DataFrame Subsetting, the .drop_duplicates() method, the .apply() method and how to save your new dataframe to a .CSV file.\n",
    "- Learn how to create groups of keywords using FuzzyWuzzy + a custom keyword grouping function\n",
    "- Learn how to use  <strong> .lower() </strong> to improve de-duplication\n",
    "- Learn how to use  <strong> NLP lemmmatization </strong> to improve de-duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicating Keywords With Set Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set operations allow us to easily de-duplicate a Python list which contains exact duplicates like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transform the set back into a Python list and re-assign it back to the original variable keyword_list_example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of using this type of de-duplication is that it is natively supported within Python, however it doesn't allow us to capture partial matches. That's where <strong> FuzzyWuzzy comes in! </strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Compare Two Keyword Lists With Set Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to compare two keyword lists to find where there are matches between the two lists. For example if we were to take google search console data and paid search data we would like to find the following:\n",
    "    \n",
    "\n",
    "- Items that occur within both lists\n",
    "- Items that occur in list_a but not in list_b\n",
    "- Items that occur in list_b but not in list_a\n",
    "- All of the items\n",
    "- Are all of the items in list_a in list_b (boolean - True / False)?\n",
    "- Are all of the items in list_b in list_a (boolean - True / False)?\n",
    "\n",
    "Using the power of python sets we can easily find exact matches between two python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .intersection() functionality allows us to find keywords that appear in both lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows all of the keywords within google_search_console_keywords that are not within the paid search keywords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows the exact opposite, all of the keywords that appear within the paid search keywords list that aren't in the google search console keywords list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily extract all of the total exact match keywords from both lists with the following set operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below commands allow us to search to see if all elemnents within the google_search_console keywords list are within the \n",
    "paid_search_keywords list and vice versa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplicating Keywords With [FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's [fuzzywuzzy?](https://github.com/seatgeek/fuzzywuzzy) Its a string matching python package that allows us to easily calculate the difference between several strings via [Levenshtein Distance.](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
    "\n",
    "Firstly we'll need to install the python package called fuzzywuzzy with:\n",
    "\n",
    "~~~\n",
    "pip install fuzzywuzzy\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "As a side-note, anytime you install python packages you will need to restart the python ikernel to use them within a Jupyter Notebook <strong> (click Kernel at the top, then click Restart & Clear Output). </strong>\n",
    "\n",
    "---\n",
    "\n",
    "One of the most useful functions from the [fuzzywuzzy package](https://github.com/seatgeek/fuzzywuzzy) is the .dedupe() function, so let's see it in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesphoenix/opt/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's important to notice here is that the longer phrases have been chosen as the de-duplicated keywords. Whilst this approach will provide us with a list of duplicates it does come at the cost of loosing what keywords were lost whilst doing the de-duplication.\n",
    "\n",
    "To get the original of keywords, we can take the de-duplicated dict_keys and simply transform it into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also its important to notice that there is a threshold argument which we can pass into process.dedupe:\n",
    "\n",
    "~~~\n",
    "process.dedupe(keyword_list, threshold=70)\n",
    "~~~\n",
    "\n",
    "This number can range from 0 - 100, the intuition behind it is that as the threshold increases we are only de-duplicating keywords that are more closely related to each other. Therefore if we set a low threshold, we will get more de-duplication, however it might come at the cost of quality de-duplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a balance to using the threshold parameter and I encourage you to try different different threshold values. So let's do just that! :) \n",
    "\n",
    "We can use the following code to loop over a range of numbers (stepping up with increments of 10):\n",
    "\n",
    "~~~\n",
    "\n",
    "for i in range(10, 100, 10):\n",
    "    print(i)\n",
    "    # This will start at the number 10 and will increment in steps of 10 up to 90\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Open An Ahrefs CSV Keyword Report With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a sample report from the ahrefs keyword explorer for the term \"digital marketing\". We'll be using this to show you a few ways to doing keyword data analysis + grouping within pandas. Firstly we can load the CSV file with the following syntax:\n",
    "\n",
    "~~~\n",
    "\n",
    "df = pd.read_csv('csv_file_path.csv')\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Important Points: </strong>\n",
    "\n",
    "- All of the keyword reports from Ahrefs are tab seperated, this will need to be specified when we read the CSV file.\n",
    "- Depending upon how you download keyword.csv's from Ahrefs will require a different type of encoding. Again this can be specified inside of the pd.read_csv() function.\n",
    "\n",
    "\n",
    "Examples:\n",
    "\n",
    "~~~\n",
    "\n",
    "df = pd.read_csv('data/digital-marketing-keyword-ideas.csv', encoding='UTF-16', delimiter='\\t')\n",
    "df = pd.read_csv('data/digital-marketing-keyword-ideas.csv', encoding='UTF-8', delimiter='\\t')\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "\n",
    ".info() allows us to inspect and see how many np.nan values are inside of the dataframe\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes are great and all of the common operations that you were previously implementing in gsheets can be completely autoamted within Pandas. Let's see how to select single and multiple columns:\n",
    "\n",
    "~~~\n",
    "\n",
    "df['single_column'] --> This will return a pd.series object which is essentially a single column.\n",
    "df[['column_one', 'column_two']] --> This will return a dataframe object, similar to the original df.\n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Index Specific Columns And Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways that you can index columns either with .loc or with .iloc:\n",
    "\n",
    "~~~\n",
    "\n",
    ".loc[] refers to the column and index names\n",
    ".iloc refers to the index position of the column and index\n",
    "\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "Remember when it comes to indexing your dataframe the order is <strong> first ROWS, then COLUMNS </strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Dataframes By Column Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rank the keywords by the organic monthly search volume in descending order, also notice that I've used inplace=True, this means that the pandas dataframe is permanently sorted by the search volume:\n",
    "\n",
    "~~~\n",
    "\n",
    "df.sort_values(by='column_name', ascending=Boolean, inplace=Boolean)\n",
    "df.head(integer)\n",
    "\n",
    "- The df.head() command allows us to easily to show the top N results.\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sort the dataframe based upon CPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay that's great, but as Ahrefs provides a Parent Keyword column, let's firstly remove any keywords that don't have a value for this column:\n",
    "\n",
    "~~~\n",
    "\n",
    ".dropna(subset=['column_name']) This command allows us to drop np.NaN (not a number) values from the dataframe.\n",
    "\n",
    "~~~\n",
    "\n",
    "Also let's remove the # column as it is unnecessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilising GroupBy Objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a Pandas function called .groupby() which will allow us to group keywords based upon their Parent Keyword:\n",
    "    \n",
    "~~~\n",
    "\n",
    "df.groupby('column_name')\n",
    "\n",
    "~~~\n",
    "\n",
    "We will also save this groupby object to a variable so that we can reference it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as it is grouping a series of keywords by their Parent Keyword, we need to use aggregation to summarise the grouped metrics. Common functions include the following:\n",
    "\n",
    "~~~\n",
    "\n",
    ".mean()\n",
    ".count()\n",
    ".sum()\n",
    ".median()\n",
    "\n",
    "~~~\n",
    "\n",
    "However for our analysis we'll want to use a custom .agg() function so that we can apply different summarisation techniques to unique columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Do Section:\n",
    "\n",
    "Now we can use the similar filtering techniques that we learned earlier. Code along and filter the groupedby dataframe by: \n",
    "\n",
    "- Difficulty\n",
    "- Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now return to our original dataframe and practice some other useful methods. Firstly we can attempt to drop any duplicates within our keyword column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However as there are no duplicates inside of the keywords column, let's get a de-duplicated list of parent keywords by using the following command:\n",
    "\n",
    "~~~\n",
    "\n",
    ".drop_duplicates(subset=['column_name'])\n",
    "\n",
    "~~~\n",
    "\n",
    "Then we will select the Parent Keyword column, and convert it into a python list with:\n",
    "\n",
    "~~~\n",
    "\n",
    ".tolist()\n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Filtering (subsetting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest type of filtering is to use the = operator. \n",
    "\n",
    "For example, we might want to find keywords that are equal to hubspot within our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also write the same filter like so:\n",
    "    \n",
    "~~~\n",
    "\n",
    "single_keyword_df = df[df['Keyword'] == 'hubspot']\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally might want to filter our dataframe to only find keywords that have over a specific search volume greater than 50, so let's do just that:\n",
    "\n",
    "~~~\n",
    "\n",
    "df['column_name'] > 50\n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another short handed way to accomplish the same operation would be:\n",
    "\n",
    "~~~\n",
    "\n",
    "filtered_dataframe  = df[df['Volume'] > 50]\n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter the dataframe by several columns by chaining the boolean dataframe subsets together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could also be written like this:\n",
    "\n",
    "~~~\n",
    "\n",
    "two_column_filtered_dataframe = df[(df['Volume'] > 50) & (df['CPC'] > 2.0)]\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do OR operations with the pipe operator <strong> | </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which could also be written like this:\n",
    "    \n",
    "~~~\n",
    "\n",
    "two_column_filtered_dataframe = df[(df['Volume'] > 50) | (df['CPC'] > 2.0)]\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping de_duplicated keywords with FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as de-duplicating lists of keywords, it would also be useful to keep all of the keywords but bucket them into keyword groups based upon how close every keyword was as a duplicate in reference to every keyword. \n",
    "\n",
    "Without going into the specifics of how this functions work, you can use it as a way to group keywords based upon their FuzzyWuzzy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_grouper(df, column_name=None, limit=6, threshold=85):\n",
    "    # Create a near_match_duplicated_list \n",
    "    test = df.drop_duplicates(subset=column_name)[column_name].tolist()\n",
    "    master_dict = {}\n",
    "    processed_words = []\n",
    "    no_matches = []\n",
    "\n",
    "    for index, item in enumerate(df[column_name]):\n",
    "        # Let's pop out the first index from the list so we never match against \n",
    "        try:\n",
    "            test.pop(0)\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "        \n",
    "        # Let's only loop over the keywords that aren't already grouped\n",
    "        if item not in processed_words:\n",
    "            # Creating the top N matches\n",
    "            try:\n",
    "                matches = process.extract(item, test, limit=limit)\n",
    "                \"\"\"Extracting out the matched words - A threshold for this can be changed so that \n",
    "                    we never cluster words together with a low match score\"\"\"\n",
    "                matches = [item for item in matches if item[1] > threshold]\n",
    "                matched_words = [item[0] for item in matches if item[1] > threshold]\n",
    "                # Saving the matches to a dictionary\n",
    "                master_dict[item] = matches\n",
    "                # Saving the matches and  to a list of processed words\n",
    "                processed_words.extend(matched_words)\n",
    "            except Exception as e:\n",
    "                no_matches.append(item)\n",
    "        else:\n",
    "            pass\n",
    "    return master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ways That We Can Improve Our De-Duplication Efforts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using .lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".lower() on a list of strings ensures that any strings which are duplicates such as \"digital marketing services\" vs \"Digital Marketing Services\" can be normalised with this in-built function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stemming + Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily I've already got some stemming + lemmatized functions that we can utilise on our previous dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we'll need to install the python package called NLTK with:\n",
    "\n",
    "~~~\n",
    "pip install nltk\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "# NLP Processing\n",
    "def stem_sentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    # token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#rejoin words after stemming\n",
    "def rejoin_words(row):\n",
    "    joined_words = (\" \".join(row))\n",
    "    return joined_words\n",
    "\n",
    "#prepare list of all words in criteria report\n",
    "def prepare_list(df, column):\n",
    "    results = []\n",
    "    for t in df[column]:\n",
    "        x=t.split()\n",
    "        for i in x:\n",
    "            results.append(i)\n",
    "    # Remove Duplicates:           \n",
    "    return list(set(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use an .apply() method on the dataframe, basically how this method works is that it will perform an operation on either every row, or every column. One at a time! \n",
    "\n",
    "For example, let's do a very simple method using the above function (test). Ths will simply return every row:\n",
    "\n",
    "~~~\n",
    "\n",
    "df['Keyword'].apply(test)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      coast\n",
       "1                                    hubspot\n",
       "2                          digital marketing\n",
       "3                                    digital\n",
       "4                            content meaning\n",
       "                       ...                  \n",
       "995         email marketing digital services\n",
       "996      all digital marketing services list\n",
       "997    wow search digital marketing services\n",
       "998       digital marketing services in kota\n",
       "999        digital marketing services career\n",
       "Name: Keyword, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply some stemming to our keyword column with an .apply() method: \n",
    "\n",
    "- Then we will save it as a new column by re-assigning it to the original dataframe with a new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use these stemmed/lemmatized keywords instead of the original keywords whilst performing de-duplication with FuzzyWuzzy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Country</th>\n",
       "      <th>Difficulty</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CPC</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>CPS</th>\n",
       "      <th>Return Rate</th>\n",
       "      <th>Parent Keyword</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>SERP Features</th>\n",
       "      <th>Stemmed Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>coast</td>\n",
       "      <td>gb</td>\n",
       "      <td>42.0</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>64715.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.67</td>\n",
       "      <td>coast</td>\n",
       "      <td>2020-05-09 23:55:39</td>\n",
       "      <td>Sitelinks</td>\n",
       "      <td>coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>hubspot</td>\n",
       "      <td>gb</td>\n",
       "      <td>67.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>59708.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.34</td>\n",
       "      <td>hubspot</td>\n",
       "      <td>2020-05-09 00:57:01</td>\n",
       "      <td>Adwords top, Sitelinks, Top stories, Thumbnail...</td>\n",
       "      <td>hubspot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>digital marketing</td>\n",
       "      <td>gb</td>\n",
       "      <td>74.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11033.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.41</td>\n",
       "      <td>digital marketing</td>\n",
       "      <td>2020-05-09 07:11:09</td>\n",
       "      <td>Adwords top, Sitelinks, People also ask, Top s...</td>\n",
       "      <td>digit market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #            Keyword Country  Difficulty   Volume  CPC   Clicks   CPS  \\\n",
       "0  1              coast      gb        42.0  70000.0  2.5  64715.0  0.93   \n",
       "1  2            hubspot      gb        67.0  63000.0  5.0  59708.0  0.95   \n",
       "2  3  digital marketing      gb        74.0  19000.0  7.0  11033.0  0.57   \n",
       "\n",
       "   Return Rate     Parent Keyword          Last Update  \\\n",
       "0         1.67              coast  2020-05-09 23:55:39   \n",
       "1         2.34            hubspot  2020-05-09 00:57:01   \n",
       "2         1.41  digital marketing  2020-05-09 07:11:09   \n",
       "\n",
       "                                       SERP Features Stemmed Keyword  \n",
       "0                                          Sitelinks          coast   \n",
       "1  Adwords top, Sitelinks, Top stories, Thumbnail...        hubspot   \n",
       "2  Adwords top, Sitelinks, People also ask, Top s...   digit market   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is how you could filter the dataframe to return uniques with the .isin() function:\n",
    "\n",
    "~~~\n",
    "\n",
    "df[df['Stemmed Keyword'].isin(python_list)]\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also you will notice that the index on the rows is not completely reset:\n",
    "\n",
    "We can reset the index with:\n",
    "\n",
    "~~~\n",
    "\n",
    "unique_stemmed_df.reset_index()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our Final Dataframe To CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've performed any data analysis or script, you can easily save the pandas series object or pandas dataframe to CSV with:\n",
    "\n",
    "~~~\n",
    "\n",
    "df.to_csv('name_of_file.csv', index=True)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've made it to the end of the first tutorial! \n",
    "\n",
    "We've introduced you to the following frameworks:\n",
    "\n",
    "- Python Sets\n",
    "- FuzzyWuzzy\n",
    "- Pandas\n",
    "- NLP Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its time to continue on our epic journey and to learn more scripts which will help to automate your SEO life! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
